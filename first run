Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1084-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Tue Jul 15 03:08:04 UTC 2025

  System load:  0.05               Processes:             108
  Usage of /:   45.3% of 24.16GB   Users logged in:       0
  Memory usage: 6%                 IPv4 address for eth0: 172.31.95.162
  Swap usage:   0%

 * Ubuntu Pro delivers the most comprehensive open source security and
   compliance features.

   https://ubuntu.com/aws/pro

Expanded Security Maintenance for Infrastructure is not enabled.

55 updates can be applied immediately.
To see these additional updates run: apt list --upgradable

Enable ESM Infra to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status


The list of available updates is more than a week old.
To check for new updates run: sudo apt update

Last login: Thu Jul 10 07:08:38 2025 from 18.206.107.28
root@ip-172-31-95-162:~# sudo apt update
Hit:1 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal InRelease
Get:2 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]
Get:3 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]
Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]
Get:5 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3954 kB]
Get:6 http://us-east-1.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1262 kB]
Get:7 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3564 kB]
Get:8 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [518 kB]
Get:9 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1040 kB]
Get:10 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [221 kB]
Fetched 10.9 MB in 2s (5290 kB/s)                           
Reading package lists... Done
Building dependency tree       
Reading state information... Done
67 packages can be upgraded. Run 'apt list --upgradable' to see them.
root@ip-172-31-95-162:~# sudo su - hadoop
hadoop@ip-172-31-95-162:~$ start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hadoop in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [master]
Starting datanodes
Starting secondary namenodes [ip-172-31-95-162]
Starting resourcemanager
Starting nodemanagers
hadoop@ip-172-31-95-162:~$ cd IST3134/
hadoop@ip-172-31-95-162:~/IST3134$ mkdir ~/stackoverflow
hadoop@ip-172-31-95-162:~/IST3134$ ls
avgwordlength  avgwordlength.zip  shakespeare  shakespeare.zip  wordcount  wordcount.zip
hadoop@ip-172-31-95-162:~/IST3134$ cd ~/stackoverflow
hadoop@ip-172-31-95-162:~/stackoverflow$ aws s3 cp s3://stackoverflow-1/Questions.csv
usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]
To see help text, you can run:

  aws help
  aws <command> help
  aws <command> <subcommand> help
aws: error: the following arguments are required: paths
hadoop@ip-172-31-95-162:~/stackoverflow$ aws s3 cp s3://stackoverflow-1/Questions.csv/ --recursive
usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]
To see help text, you can run:

  aws help
  aws <command> help
  aws <command> <subcommand> help
aws: error: the following arguments are required: paths
hadoop@ip-172-31-95-162:~/stackoverflow$ aws s3 cp Questions s3://stackoverflow-1/Questions.csv/ --recursive

The user-provided path Questions does not exist.
hadoop@ip-172-31-95-162:~/stackoverflow$ ^C
hadoop@ip-172-31-95-162:~/stackoverflow$ cd IST3134
-bash: cd: IST3134: No such file or directory
hadoop@ip-172-31-95-162:~/stackoverflow$ cd ~/IST3134
hadoop@ip-172-31-95-162:~/IST3134$ aws s3 cp stackoverflow s3://stackoverflow-1/Questions.csv/ --recursive

The user-provided path stackoverflow does not exist.
hadoop@ip-172-31-95-162:~/IST3134$ ls
avgwordlength  avgwordlength.zip  shakespeare  shakespeare.zip  wordcount  wordcount.zip
hadoop@ip-172-31-95-162:~/IST3134$ ..
..: command not found
hadoop@ip-172-31-95-162:~/IST3134$ cd ..
hadoop@ip-172-31-95-162:~$ ls
Downloads  data    hadoop-3.2.2  hive-3.1.2   pig-0.17.0  spark-3.1.2              stackoverflow  workspace
IST3134    hadoop  hadoop-3.3.6  oozie-5.2.1  spark       spark-3.4.1-bin-hadoop3  workplace
hadoop@ip-172-31-95-162:~$ cd stackoverflow
hadoop@ip-172-31-95-162:~/stackoverflow$ aws s3 cp s3://stackoverflow-1/Questions.csv
usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]
To see help text, you can run:

  aws help
  aws <command> help
  aws <command> <subcommand> help
aws: error: the following arguments are required: paths
hadoop@ip-172-31-95-162:~/stackoverflow$ aws s3 ls s3://stackoverflow-1/Questions.csv
2025-07-15 02:51:30 1923682009 Questions.csv
hadoop@ip-172-31-95-162:~/stackoverflow$ aws s3 cp s3://stackoverflow-1/Questions.csv ~/stackoverflow
download: s3://stackoverflow-1/Questions.csv to ./Questions.csv   
hadoop@ip-172-31-95-162:~/stackoverflow$ nano WordCountMapper.java
hadoop@ip-172-31-95-162:~/stackoverflow$ nano WordCountReducer.java
hadoop@ip-172-31-95-162:~/stackoverflow$ ls
Questions.csv  WordCountMapper.java  WordCountReducer.java
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop classpath
/home/hadoop/hadoop/etc/hadoop:/home/hadoop/hadoop/share/hadoop/common/lib/*:/home/hadoop/hadoop/share/hadoop/common/*:/home/hadoop/hadoop/share/hadoop/hdfs:/home/hadoop/hadoop/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop/share/hadoop/hdfs/*:/home/hadoop/hadoop/share/hadoop/mapreduce/*:/home/hadoop/hadoop/share/hadoop/yarn:/home/hadoop/hadoop/share/hadoop/yarn/lib/*:/home/hadoop/hadoop/share/hadoop/yarn/*
hadoop@ip-172-31-95-162:~/stackoverflow$ mkdir ~/stackoverflow/stubs/
hadoop@ip-172-31-95-162:~/stackoverflow$ ls
Questions.csv  WordCountMapper.java  WordCountReducer.java  stubs
hadoop@ip-172-31-95-162:~/stackoverflow$ mv WordCountMapper.java WordCountReducer.java stubs/
hadoop@ip-172-31-95-162:~/stackoverflow$ ls
Questions.csv  stubs
hadoop@ip-172-31-95-162:~/stackoverflow$ javac -classpath `hadoop classpath` stubs/*.java
hadoop@ip-172-31-95-162:~/stackoverflow$ jar cvf so.jar stubs/*.class
added manifest
adding: stubs/WordCountMapper.class(in = 2541) (out= 1152)(deflated 54%)
adding: stubs/WordCountReducer.class(in = 1602) (out= 669)(deflated 58%)
hadoop@ip-172-31-95-162:~/stackoverflow$ ls
Questions.csv  so.jar  stubs
hadoop@ip-172-31-95-162:~/stackoverflow$ ls stubs
WordCountMapper.class  WordCountMapper.java  WordCountReducer.class  WordCountReducer.java
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop jar so.jar stubs.WordCount stackoverflow wordcounts
Exception in thread "main" java.lang.ClassNotFoundException: stubs.WordCount
        at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:321)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:241)
hadoop@ip-172-31-95-162:~/stackoverflow$ cd ~/stubs
-bash: cd: /home/hadoop/stubs: No such file or directory
hadoop@ip-172-31-95-162:~/stackoverflow$ cd stubs
hadoop@ip-172-31-95-162:~/stackoverflow/stubs$ cd stackoverflow
-bash: cd: stackoverflow: No such file or directory
hadoop@ip-172-31-95-162:~/stackoverflow/stubs$ cd ~/stackoverflow
hadoop@ip-172-31-95-162:~/stackoverflow$ nano WordCount.java
hadoop@ip-172-31-95-162:~/stackoverflow$ mv WordCount.java stubs/
hadoop@ip-172-31-95-162:~/stackoverflow$ ls stubs
WordCount.java  WordCountMapper.class  WordCountMapper.java  WordCountReducer.class  WordCountReducer.java
hadoop@ip-172-31-95-162:~/stackoverflow$ -rm WordCountMapper.class WordCountReducer.class stubs/

Command '-rm' not found, did you mean:

  command 'vrm' from deb atfs (1.4pl6-14)
  command 'crm' from deb crmsh (4.2.0-2ubuntu1.1)
  command 'crm' from deb crm114 (20100106-9)
  command 'srm' from deb secure-delete (3.1-6ubuntu2)
  command 'rm' from deb coreutils (8.30-3ubuntu2)
  command 'frm' from deb mailutils (1:3.7-2.1)

Try: apt install <deb name>

hadoop@ip-172-31-95-162:~/stackoverflow$ rm WordCountMapper.class WordCountReducer.class stubs/
rm: cannot remove 'WordCountMapper.class': No such file or directory
rm: cannot remove 'WordCountReducer.class': No such file or directory
rm: cannot remove 'stubs/': Is a directory
hadoop@ip-172-31-95-162:~/stackoverflow$ rm -r WordCountMapper.class WordCountReducer.class stubs/
rm: cannot remove 'WordCountMapper.class': No such file or directory
rm: cannot remove 'WordCountReducer.class': No such file or directory
hadoop@ip-172-31-95-162:~/stackoverflow$ ls stubs
ls: cannot access 'stubs': No such file or directory
hadoop@ip-172-31-95-162:~/stackoverflow$ ls
Questions.csv  so.jar
hadoop@ip-172-31-95-162:~/stackoverflow$ nano WordCountMapper.java
hadoop@ip-172-31-95-162:~/stackoverflow$ nano WordCountReducer.java
hadoop@ip-172-31-95-162:~/stackoverflow$ javac -classpath `hadoop classpath` stubs/*.java
javac: file not found: stubs/*.java
Usage: javac <options> <source files>
use -help for a list of possible options
hadoop@ip-172-31-95-162:~/stackoverflow$ nano WordCount.java
hadoop@ip-172-31-95-162:~/stackoverflow$ mkdir ~stackoverflow/stubs/
mkdir: cannot create directory ‘~stackoverflow/stubs/’: No such file or directory
hadoop@ip-172-31-95-162:~/stackoverflow$ mkdir ~/stackoverflow/stubs
hadoop@ip-172-31-95-162:~/stackoverflow$ ls
Questions.csv  WordCount.java  WordCountMapper.java  WordCountReducer.java  so.jar  stubs
hadoop@ip-172-31-95-162:~/stackoverflow$ javac -classpath `hadoop classpath` stubs/*.java
javac: file not found: stubs/*.java
Usage: javac <options> <source files>
use -help for a list of possible options
hadoop@ip-172-31-95-162:~/stackoverflow$ mv WordCountMapper.java WordCountReducer.java WordCount.java stubs/
hadoop@ip-172-31-95-162:~/stackoverflow$ ls stubs
WordCount.java  WordCountMapper.java  WordCountReducer.java
hadoop@ip-172-31-95-162:~/stackoverflow$ javac -classpath `hadoop classpath` stubs/*.java
hadoop@ip-172-31-95-162:~/stackoverflow$ ls stubs
WordCount.class  WordCount.java  WordCountMapper.class  WordCountMapper.java  WordCountReducer.class  WordCountReducer.java
hadoop@ip-172-31-95-162:~/stackoverflow$ jar cvf so.jar stubs/*.class
added manifest
adding: stubs/WordCount.class(in = 1483) (out= 796)(deflated 46%)
adding: stubs/WordCountMapper.class(in = 2547) (out= 1157)(deflated 54%)
adding: stubs/WordCountReducer.class(in = 1608) (out= 674)(deflated 58%)
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop jar so.jar stubs.WordCount stackoverflow wordcounts
2025-07-15 04:08:23,553 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.31.95.162:8032
2025-07-15 04:08:24,134 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2025-07-15 04:08:24,170 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1752549079094_0001
2025-07-15 04:08:25,313 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1752549079094_0001
Exception in thread "main" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://master:9000/user/hadoop/stackoverflow
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:404)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:310)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:327)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200)
        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1678)
        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1675)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1675)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1696)
        at stubs.WordCount.main(WordCount.java:36)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:328)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:241)
Caused by: java.io.IOException: Input path does not exist: hdfs://master:9000/user/hadoop/stackoverflow
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)
        ... 19 more
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -put stackoverflow /user/hadoop/stackoverflow/Questions.csv
put: `/user/hadoop/stackoverflow/Questions.csv': No such file or directory: `hdfs://master:9000/user/hadoop/stackoverflow/Questions.csv'
hadoop@ip-172-31-95-162:~/stackoverflow$ ls
Questions.csv  so.jar  stubs
hadoop@ip-172-31-95-162:~/stackoverflow$ hdfs dfs -put Questions.csv /user/hadoop/stackoverflow/
put: `/user/hadoop/stackoverflow/': No such file or directory: `hdfs://master:9000/user/hadoop/stackoverflow'
hadoop@ip-172-31-95-162:~/stackoverflow$ hdfs dfs -put Questions.csv /user/hadoop/stackoverflow

hadoop@ip-172-31-95-162:~/stackoverflow$ 
hadoop@ip-172-31-95-162:~/stackoverflow$ hdfs fs -put Questions.csv /user/hadoop/stackoverflow
ERROR: fs is not COMMAND nor fully qualified CLASSNAME.
Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]

  OPTIONS is none or any of:

--buildpaths                       attempt to add class files from build tree
--config dir                       Hadoop config directory
--daemon (start|status|stop)       operate on a daemon
--debug                            turn on shell script debug mode
--help                             usage information
--hostnames list[,of,host,names]   hosts to use in worker mode
--hosts filename                   list of hosts to use in worker mode
--loglevel level                   set the log4j level for this command
--workers                          turn on worker mode

  SUBCOMMAND is one of:


    Admin Commands:

cacheadmin           configure the HDFS cache
crypto               configure HDFS encryption zones
debug                run a Debug Admin to execute HDFS debug commands
dfsadmin             run a DFS admin client
dfsrouteradmin       manage Router-based federation
ec                   run a HDFS ErasureCoding CLI
fsck                 run a DFS filesystem checking utility
haadmin              run a DFS HA admin client
jmxget               get JMX exported values from NameNode or DataNode.
oev                  apply the offline edits viewer to an edits file
oiv                  apply the offline fsimage viewer to an fsimage
oiv_legacy           apply the offline fsimage viewer to a legacy fsimage
storagepolicies      list/get/set/satisfyStoragePolicy block storage policies

    Client Commands:

classpath            prints the class path needed to get the hadoop jar and the required libraries
dfs                  run a filesystem command on the file system
envvars              display computed Hadoop environment variables
fetchdt              fetch a delegation token from the NameNode
getconf              get config values from configuration
groups               get the groups which users belong to
lsSnapshottableDir   list all snapshottable dirs owned by the current user
snapshotDiff         diff two snapshots of a directory or diff the current directory contents with a snapshot
version              print the version

    Daemon Commands:

balancer             run a cluster balancing utility
datanode             run a DFS datanode
dfsrouter            run the DFS router
diskbalancer         Distributes data evenly among disks on a given node
httpfs               run HttpFS server, the HDFS HTTP Gateway
journalnode          run the DFS journalnode
mover                run a utility to move block replicas across storage types
namenode             run the DFS namenode
nfs3                 run an NFS version 3 gateway
portmap              run a portmap service
secondarynamenode    run the DFS secondary namenode
sps                  run external storagepolicysatisfier
zkfc                 run the ZK Failover Controller daemon

SUBCOMMAND may print help when invoked w/o parameters or with -h.
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -put Questions.csv /user/hadoop/stackoverflow
put: `/user/hadoop/stackoverflow': File exists
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop jar so.jar stubs.WordCount stackoverflow wordcounts
2025-07-15 04:14:02,314 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at master/172.31.95.162:8032
2025-07-15 04:14:02,776 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2025-07-15 04:14:02,803 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1752549079094_0002
2025-07-15 04:14:03,101 INFO input.FileInputFormat: Total input files to process : 1
2025-07-15 04:14:03,219 INFO mapreduce.JobSubmitter: number of splits:15
2025-07-15 04:14:03,455 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1752549079094_0002
2025-07-15 04:14:03,456 INFO mapreduce.JobSubmitter: Executing with tokens: []
2025-07-15 04:14:03,692 INFO conf.Configuration: resource-types.xml not found
2025-07-15 04:14:03,692 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2025-07-15 04:14:04,160 INFO impl.YarnClientImpl: Submitted application application_1752549079094_0002
2025-07-15 04:14:04,204 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1752549079094_0002/
2025-07-15 04:14:04,205 INFO mapreduce.Job: Running job: job_1752549079094_0002
2025-07-15 04:14:12,379 INFO mapreduce.Job: Job job_1752549079094_0002 running in uber mode : false
2025-07-15 04:14:12,380 INFO mapreduce.Job:  map 0% reduce 0%
2025-07-15 04:14:39,596 INFO mapreduce.Job:  map 20% reduce 0%
2025-07-15 04:14:41,621 INFO mapreduce.Job:  map 23% reduce 0%
2025-07-15 04:14:42,628 INFO mapreduce.Job:  map 40% reduce 0%
2025-07-15 04:14:44,639 INFO mapreduce.Job:  map 49% reduce 0%
2025-07-15 04:14:45,644 INFO mapreduce.Job:  map 55% reduce 0%
2025-07-15 04:14:50,678 INFO mapreduce.Job:  map 63% reduce 0%
2025-07-15 04:14:51,683 INFO mapreduce.Job:  map 75% reduce 0%
2025-07-15 04:14:53,695 INFO mapreduce.Job:  map 91% reduce 0%
2025-07-15 04:14:54,703 INFO mapreduce.Job:  map 100% reduce 0%
2025-07-15 04:14:55,707 INFO mapreduce.Job:  map 100% reduce 100%
2025-07-15 04:14:55,714 INFO mapreduce.Job: Job job_1752549079094_0002 completed successfully
2025-07-15 04:14:55,817 INFO mapreduce.Job: Counters: 55
        File System Counters
                FILE: Number of bytes read=146703
                FILE: Number of bytes written=4713696
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1923740988
                HDFS: Number of bytes written=92383
                HDFS: Number of read operations=50
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
                HDFS: Number of bytes read erasure-coded=0
        Job Counters 
                Killed map tasks=1
                Launched map tasks=16
                Launched reduce tasks=1
                Data-local map tasks=16
                Total time spent by all maps in occupied slots (ms)=492463
                Total time spent by all reduces in occupied slots (ms)=10970
                Total time spent by all map tasks (ms)=492463
                Total time spent by all reduce tasks (ms)=10970
                Total vcore-milliseconds taken by all map tasks=492463
                Total vcore-milliseconds taken by all reduce tasks=10970
                Total megabyte-milliseconds taken by all map tasks=504282112
                Total megabyte-milliseconds taken by all reduce tasks=11233280
        Map-Reduce Framework
                Map input records=42420636
                Map output records=7088
                Map output bytes=132440
                Map output materialized bytes=146787
                Input split bytes=1635
                Combine input records=0
                Combine output records=0
                Reduce input groups=4660
                Reduce shuffle bytes=146787
                Reduce input records=7088
                Reduce output records=4660
                Spilled Records=14176
                Shuffled Maps =15
                Failed Shuffles=0
                Merged Map outputs=15
                GC time elapsed (ms)=5871
                CPU time spent (ms)=66950
                Physical memory (bytes) snapshot=6085603328
                Virtual memory (bytes) snapshot=40669937664
                Total committed heap usage (bytes)=5317328896
                Peak Map Physical memory (bytes)=480907264
                Peak Map Virtual memory (bytes)=2543980544
                Peak Reduce Physical memory (bytes)=237514752
                Peak Reduce Virtual memory (bytes)=2548973568
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=1923739353
        File Output Format Counters 
                Bytes Written=92383
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -ls wordcounts
Found 2 items
-rw-r--r--   2 hadoop supergroup          0 2025-07-15 04:14 wordcounts/_SUCCESS
-rw-r--r--   2 hadoop supergroup      92383 2025-07-15 04:14 wordcounts/part-r-00000
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -cat wordcounts
cat: `wordcounts': Is a directory
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -cat
-cat: Not enough arguments: expected 1 but got 0
Usage: hadoop fs [generic options]
        [-appendToFile [-n] <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-checksum [-v] <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-concat <target path> <src path> <src path> ...]
        [-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]
        [-copyToLocal [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]
        [-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] [-s] <path> ...]
        [-cp [-f] [-p | -p[topax]] [-d] [-t <thread count>] [-q <thread pool queue size>] <src> ... <dst>]
        [-createSnapshot <snapshotDir> [<snapshotName>]]
        [-deleteSnapshot <snapshotDir> <snapshotName>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] [-v] [-x] <path> ...]
        [-expunge [-immediate] [-fs <path>]]
        [-find <path> ... <expression> ...]
        [-get [-f] [-p] [-crc] [-ignoreCrc] [-t <thread count>] [-q <thread pool queue size>] <src> ... <localdst>]
        [-getfacl [-R] <path>]
        [-getfattr [-R] {-n name | -d} [-e en] <path>]
        [-getmerge [-nl] [-skip-empty-file] <src> <localdst>]
        [-head <file>]
        [-help [cmd ...]]
        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] [-l] [-d] [-t <thread count>] [-q <thread pool queue size>] <localsrc> ... <dst>]
        [-renameSnapshot <snapshotDir> <oldName> <newName>]
        [-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
        [-setfattr {-n name [-v value] | -x name} <path>]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] [-s <sleep interval>] <file>]
        [-test -[defswrz] <path>]
        [-text [-ignoreCrc] <src> ...]
        [-touch [-a] [-m] [-t TIMESTAMP (yyyyMMdd:HHmmss) ] [-c] <path> ...]
        [-touchz <path> ...]
        [-truncate [-w] <length> <path> ...]
        [-usage [cmd ...]]

Generic options supported are:
-conf <configuration file>        specify an application configuration file
-D <property=value>               define a value for a given property
-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.
-jt <local|resourcemanager:port>  specify a ResourceManager
-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster
-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath
-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines

The general command line syntax is:
command [genericOptions] [commandOptions]

Usage: hadoop fs [generic options] -cat [-ignoreCrc] <src> ...
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -cat wordcounts
cat: `wordcounts': Is a directory
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -ls wordcounts
Found 2 items
-rw-r--r--   2 hadoop supergroup          0 2025-07-15 04:14 wordcounts/_SUCCESS
-rw-r--r--   2 hadoop supergroup      92383 2025-07-15 04:14 wordcounts/part-r-00000
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -cat wordcounts/part-r-00000 | less
cat: Unable to write to output stream.
hadoop@ip-172-31-95-162:~/stackoverflow$ hadoop fs -cat wordcounts/part-r-00000 | sort -k2 -nr | less
hadoop@ip-172-31-95-162:~/stackoverflow$ ^C
hadoop@ip-172-31-95-162:~/stackoverflow$ 
